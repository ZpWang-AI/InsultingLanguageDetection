{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zjdou/zpwang/InsultingLanguageDetection\n"
     ]
    }
   ],
   "source": [
    "# 选择根目录\n",
    "%cd /home/zjdou/zpwang/InsultingLanguageDetection/\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "sentence = 'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 创建分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,  # 模型名称\n",
    "    cache_dir='./pretrained_model',  # 模型保存地址，可忽略\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 2088,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,  # 模型名称\n",
    "    cache_dir='./pretrained_model/',  # 模型保存地址，可忽略\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1689,  0.1361, -0.1394,  ..., -0.6251,  0.0522,  0.3671],\n",
       "         [-0.3633,  0.1412,  0.8800,  ...,  0.1043,  0.2888,  0.3727],\n",
       "         [-0.6986, -0.6988,  0.0645,  ..., -0.2210,  0.0099, -0.5940],\n",
       "         [ 0.8310,  0.1237, -0.1512,  ...,  0.1031, -0.6779, -0.2629]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-9.0615e-01, -3.1115e-01, -6.2166e-01,  7.7409e-01,  2.8987e-01,\n",
       "         -1.9024e-01,  9.2471e-01,  1.8180e-01, -5.0504e-01, -9.9994e-01,\n",
       "         -2.3735e-01,  8.7621e-01,  9.7716e-01,  2.5809e-01,  9.3802e-01,\n",
       "         -6.9061e-01, -5.2105e-01, -5.4915e-01,  3.5759e-01, -7.5982e-01,\n",
       "          6.0571e-01,  9.9936e-01,  3.3454e-01,  2.5202e-01,  4.1586e-01,\n",
       "          9.7050e-01, -7.8427e-01,  9.3400e-01,  9.6314e-01,  6.5928e-01,\n",
       "         -7.1543e-01,  1.0239e-01, -9.8550e-01, -1.6261e-01, -6.7724e-01,\n",
       "         -9.8579e-01,  3.0743e-01, -7.8618e-01,  1.3032e-01,  1.2667e-02,\n",
       "         -8.9823e-01,  2.1339e-01,  9.9973e-01, -1.8709e-01,  2.3900e-01,\n",
       "         -2.4464e-01, -1.0000e+00,  2.8436e-01, -8.7582e-01,  7.1257e-01,\n",
       "          6.5698e-01,  5.2143e-01,  1.0539e-01,  4.5140e-01,  4.4865e-01,\n",
       "          2.7456e-01, -9.8213e-02,  9.2734e-02, -2.2094e-01, -5.4374e-01,\n",
       "         -6.0034e-01,  3.6505e-01, -6.5587e-01, -9.1238e-01,  7.8441e-01,\n",
       "          4.8337e-01, -1.0489e-01, -1.9499e-01, -4.8225e-02, -2.2996e-01,\n",
       "          8.9400e-01,  2.5066e-01,  3.8609e-01, -8.4989e-01,  3.2252e-01,\n",
       "          2.1331e-01, -5.5507e-01,  1.0000e+00, -6.5867e-01, -9.7461e-01,\n",
       "          4.9343e-01,  4.6558e-01,  4.7178e-01,  5.5720e-02,  5.1437e-02,\n",
       "         -1.0000e+00,  4.4387e-01, -7.4709e-02, -9.8641e-01,  1.6191e-01,\n",
       "          4.3324e-01, -1.6432e-01, -1.2176e-01,  4.6759e-01, -4.1206e-01,\n",
       "         -2.6535e-01, -3.0527e-01, -5.9662e-01, -2.3193e-01, -1.7307e-01,\n",
       "          1.9924e-02, -2.1107e-01, -2.4200e-01, -3.0866e-01,  2.2780e-01,\n",
       "         -4.2308e-01, -6.4542e-01,  2.6983e-01, -2.7557e-01,  6.3427e-01,\n",
       "          2.9822e-01, -3.1062e-01,  4.6449e-01, -9.6291e-01,  5.5464e-01,\n",
       "         -1.9653e-01, -9.8220e-01, -5.1419e-01, -9.8544e-01,  6.4684e-01,\n",
       "         -1.8899e-01, -2.4919e-01,  9.6180e-01,  3.8441e-01,  2.7922e-01,\n",
       "          5.9582e-02, -6.3763e-01, -1.0000e+00, -6.1973e-01, -4.0413e-01,\n",
       "          4.7590e-02, -1.9831e-01, -9.7161e-01, -9.4822e-01,  5.7882e-01,\n",
       "          9.5076e-01,  9.9642e-02,  9.9916e-01, -2.4947e-01,  9.3731e-01,\n",
       "         -2.0715e-01, -5.7406e-01,  4.2462e-01, -4.3113e-01,  6.4491e-01,\n",
       "          4.8150e-01, -7.2302e-01,  1.0609e-01, -2.4969e-02,  3.2980e-01,\n",
       "         -5.0799e-01, -1.4917e-01, -4.5892e-01, -9.3619e-01, -3.1903e-01,\n",
       "          9.4843e-01, -2.9770e-01, -7.2527e-01,  2.2589e-01, -1.5897e-01,\n",
       "         -4.9687e-01,  8.6441e-01,  6.2593e-01,  2.7812e-01, -2.0720e-01,\n",
       "          3.9699e-01,  6.0161e-02,  5.5732e-01, -8.5815e-01, -1.8943e-02,\n",
       "          3.6645e-01, -2.3459e-01, -4.7094e-01, -9.7639e-01, -2.8612e-01,\n",
       "          4.0802e-01,  9.8942e-01,  7.3076e-01,  2.1438e-01,  5.9475e-01,\n",
       "         -1.3386e-01,  6.8417e-01, -9.4795e-01,  9.7213e-01, -2.9630e-01,\n",
       "          2.2191e-01,  3.1566e-01,  1.7508e-01, -8.5796e-01, -1.5590e-01,\n",
       "          8.6525e-01, -5.4981e-01, -8.6927e-01,  6.0578e-02, -4.4714e-01,\n",
       "         -3.6194e-01, -4.5069e-01,  4.6971e-01, -2.8228e-01, -2.6582e-01,\n",
       "         -2.3449e-02,  8.9092e-01,  9.8083e-01,  8.0408e-01, -2.4909e-02,\n",
       "          6.8485e-01, -9.0679e-01, -5.1557e-01,  5.2714e-02,  2.0982e-01,\n",
       "          1.7816e-01,  9.9394e-01, -3.0574e-01, -1.3667e-01, -9.3650e-01,\n",
       "         -9.8554e-01, -7.2891e-02, -9.1284e-01, -1.0905e-01, -6.3631e-01,\n",
       "          5.1113e-01,  3.8887e-01,  3.1968e-01,  4.0400e-01, -9.9328e-01,\n",
       "         -7.5453e-01,  2.7712e-01, -2.9502e-01,  3.3304e-01, -1.4978e-01,\n",
       "          1.2110e-01,  8.1930e-01, -4.7735e-01,  8.3838e-01,  8.9178e-01,\n",
       "         -4.5513e-01, -7.1493e-01,  8.7837e-01, -2.5635e-01,  8.8714e-01,\n",
       "         -5.9652e-01,  9.7529e-01,  7.9861e-01,  8.0444e-01, -9.1706e-01,\n",
       "         -3.1900e-01, -8.6507e-01, -4.6483e-01,  3.9681e-03, -2.7766e-01,\n",
       "          7.3487e-01,  4.5703e-01,  3.6156e-01,  6.1086e-01, -6.5924e-01,\n",
       "          9.9842e-01, -5.0613e-02, -9.5142e-01,  2.1892e-01, -1.3333e-01,\n",
       "         -9.8295e-01,  6.1005e-01,  2.8506e-01, -2.5937e-01, -4.0388e-01,\n",
       "         -5.4129e-01, -9.5388e-01,  9.2988e-01,  6.1911e-02,  9.8972e-01,\n",
       "          1.4620e-01, -9.4861e-01, -5.4303e-01, -9.0342e-01, -3.3039e-01,\n",
       "         -1.8301e-01, -3.6899e-02, -1.8631e-01, -9.5726e-01,  4.4430e-01,\n",
       "          4.2535e-01,  4.4355e-01, -3.3298e-01,  9.9867e-01,  1.0000e+00,\n",
       "          9.6086e-01,  8.8581e-01,  9.2377e-01, -9.9609e-01, -2.0700e-01,\n",
       "          9.9997e-01, -9.7150e-01, -1.0000e+00, -9.3351e-01, -6.6484e-01,\n",
       "          2.6196e-01, -1.0000e+00, -1.0932e-01,  1.7542e-01, -9.0441e-01,\n",
       "          2.9690e-01,  9.7393e-01,  9.9416e-01, -1.0000e+00,  7.8004e-01,\n",
       "          9.2378e-01, -5.6055e-01,  9.1517e-01, -2.9476e-01,  9.7083e-01,\n",
       "          4.5818e-01,  1.3447e-01, -2.2377e-01,  2.9927e-01, -7.4908e-01,\n",
       "         -8.5180e-01, -2.7196e-01, -4.1478e-01,  9.7242e-01,  8.6409e-02,\n",
       "         -7.5659e-01, -9.2212e-01,  4.7760e-02, -1.4992e-01, -2.6219e-01,\n",
       "         -9.6075e-01, -7.2093e-02,  4.1063e-01,  7.6588e-01,  3.9011e-02,\n",
       "          2.2797e-01, -7.3560e-01,  2.2872e-01, -4.5123e-01,  2.9779e-01,\n",
       "          5.9624e-01, -9.1918e-01, -5.8835e-01, -2.4553e-01, -4.3462e-01,\n",
       "         -2.3733e-01, -9.4708e-01,  9.6731e-01, -4.0816e-01,  7.5546e-01,\n",
       "          1.0000e+00, -2.8026e-02, -8.9186e-01,  5.5393e-01,  1.9125e-01,\n",
       "          8.0895e-02,  1.0000e+00,  7.3035e-01, -9.7404e-01, -4.7621e-01,\n",
       "          4.3255e-01, -4.7478e-01, -4.8012e-01,  9.9834e-01, -2.2458e-01,\n",
       "         -3.4078e-01,  8.3355e-02,  9.6681e-01, -9.8517e-01,  9.5143e-01,\n",
       "         -9.1083e-01, -9.6692e-01,  9.6374e-01,  9.3090e-01, -5.4608e-01,\n",
       "         -5.6511e-01,  7.7313e-02, -3.7943e-01,  1.8914e-01, -9.6841e-01,\n",
       "          7.6807e-01,  4.6560e-01, -4.7643e-02,  8.7445e-01, -8.9385e-01,\n",
       "         -4.4835e-01,  3.3142e-01, -5.3819e-01,  2.0117e-03,  7.1536e-01,\n",
       "          5.2621e-01, -2.8496e-01,  7.2499e-02, -3.1469e-01,  1.6666e-01,\n",
       "         -9.7549e-01,  2.1509e-01,  1.0000e+00, -5.0041e-02,  2.7363e-01,\n",
       "         -3.6835e-01,  1.4367e-02, -2.5385e-01,  4.0477e-01,  4.9355e-01,\n",
       "         -2.7157e-01, -7.9949e-01,  5.5637e-01, -9.7226e-01, -9.8250e-01,\n",
       "          8.2037e-01,  1.2428e-01, -3.1637e-01,  9.9997e-01,  4.8696e-01,\n",
       "          1.6217e-01,  2.2428e-01,  9.4464e-01, -3.0774e-02,  6.7377e-01,\n",
       "          6.1458e-01,  9.7161e-01, -1.8549e-01,  4.9363e-01,  8.6980e-01,\n",
       "         -6.6416e-01, -2.6812e-01, -6.0823e-01, -3.6964e-02, -9.0551e-01,\n",
       "          6.5654e-02, -9.5478e-01,  9.5614e-01,  7.7533e-01,  3.0881e-01,\n",
       "          1.5152e-01,  4.4798e-01,  1.0000e+00,  1.6804e-01,  6.5989e-01,\n",
       "         -6.3028e-01,  9.1261e-01, -9.9569e-01, -8.5203e-01, -3.3441e-01,\n",
       "         -1.2074e-03, -4.8370e-01, -2.5182e-01,  3.0024e-01, -9.6606e-01,\n",
       "          5.3539e-01,  3.7405e-01, -9.9356e-01, -9.9030e-01, -7.9597e-03,\n",
       "          8.9948e-01, -7.5731e-02, -9.0574e-01, -7.3163e-01, -5.7100e-01,\n",
       "          5.1156e-01, -1.4841e-01, -9.4797e-01,  7.2864e-02, -2.1514e-01,\n",
       "          4.5960e-01, -1.0345e-01,  5.0734e-01,  5.2529e-01,  7.6803e-01,\n",
       "         -6.1287e-02,  4.1251e-03, -2.1855e-02, -8.3398e-01,  8.4847e-01,\n",
       "         -8.2300e-01, -7.1640e-01, -1.4696e-01,  1.0000e+00, -4.8598e-01,\n",
       "          6.1346e-01,  7.6874e-01,  7.8394e-01, -4.5107e-02,  9.1026e-02,\n",
       "          7.6333e-01,  1.9754e-01, -4.3364e-01, -5.3293e-01, -8.5295e-01,\n",
       "         -3.3515e-01,  6.7042e-01, -1.3623e-01,  2.7169e-01,  7.9317e-01,\n",
       "          5.5210e-01,  8.3412e-02,  7.5391e-02, -7.1832e-02,  9.9966e-01,\n",
       "         -2.3658e-01, -1.0216e-01, -4.5080e-01,  8.5616e-02, -2.9959e-01,\n",
       "         -6.2786e-01,  1.0000e+00,  3.0930e-01,  1.4167e-01, -9.8768e-01,\n",
       "         -6.8458e-01, -9.1811e-01,  9.9998e-01,  8.2221e-01, -7.2770e-01,\n",
       "          6.7117e-01,  6.6115e-01, -7.5145e-02,  8.6758e-01, -1.3457e-01,\n",
       "         -2.4349e-01,  2.4777e-01,  1.0144e-01,  9.5255e-01, -4.7443e-01,\n",
       "         -9.5957e-01, -5.9315e-01,  3.3709e-01, -9.5565e-01,  9.9735e-01,\n",
       "         -4.4125e-01, -1.3977e-01, -3.6701e-01,  2.0946e-01,  7.4472e-01,\n",
       "         -6.9467e-02, -9.8080e-01, -9.7222e-02,  1.4486e-01,  9.5879e-01,\n",
       "          1.9391e-01, -4.9824e-01, -8.9137e-01,  5.8015e-01,  5.5770e-01,\n",
       "         -6.8509e-01, -9.4907e-01,  9.6072e-01, -9.8609e-01,  5.9676e-01,\n",
       "          1.0000e+00,  2.4973e-01, -3.7395e-01,  1.0501e-01, -4.4424e-01,\n",
       "          2.6123e-01, -2.3078e-01,  7.1235e-01, -9.5670e-01, -3.1889e-01,\n",
       "         -1.6605e-01,  3.2875e-01, -1.1205e-01, -3.6985e-03,  7.0291e-01,\n",
       "          1.7684e-01, -4.3112e-01, -5.8868e-01,  1.9773e-02,  3.7040e-01,\n",
       "          8.4115e-01, -2.7978e-01, -9.0355e-02,  8.1528e-02, -9.3174e-02,\n",
       "         -9.2058e-01, -1.9644e-01, -3.7062e-01, -9.9973e-01,  6.2505e-01,\n",
       "         -1.0000e+00,  1.0621e-01, -1.2206e-01, -1.6786e-01,  8.4365e-01,\n",
       "         -3.7916e-02,  3.5062e-01, -7.8014e-01, -5.3737e-01,  7.0508e-01,\n",
       "          7.8913e-01, -2.8130e-01, -4.3954e-01, -7.0514e-01,  2.4180e-01,\n",
       "         -6.6092e-03,  1.8702e-01, -4.4242e-01,  7.7169e-01, -1.5995e-01,\n",
       "          1.0000e+00,  7.2733e-02, -6.9268e-01, -9.8023e-01,  1.4577e-01,\n",
       "         -2.5391e-01,  1.0000e+00, -9.2031e-01, -9.4661e-01,  3.1159e-01,\n",
       "         -6.4689e-01, -8.4363e-01,  2.5795e-01, -4.2897e-02, -7.7705e-01,\n",
       "         -8.4594e-01,  9.5362e-01,  9.2489e-01, -4.9389e-01,  3.9911e-01,\n",
       "         -3.0106e-01, -4.7879e-01,  7.6661e-02,  5.6425e-01,  9.8501e-01,\n",
       "          1.3876e-01,  8.8985e-01,  5.5478e-01, -5.1431e-02,  9.6024e-01,\n",
       "          1.8294e-01,  5.6980e-01,  4.4824e-02,  1.0000e+00,  2.8503e-01,\n",
       "         -9.1839e-01,  2.6753e-01, -9.7828e-01, -1.6402e-01, -9.6440e-01,\n",
       "          2.8901e-01,  1.6055e-01,  8.8193e-01, -2.2969e-01,  9.5947e-01,\n",
       "         -3.7722e-01,  7.0633e-02, -6.0584e-01, -4.4726e-02,  3.6969e-01,\n",
       "         -8.9272e-01, -9.7725e-01, -9.8374e-01,  5.7585e-01, -4.3288e-01,\n",
       "          3.8860e-04,  1.3452e-01,  8.8691e-02,  2.9999e-01,  4.2328e-01,\n",
       "         -1.0000e+00,  9.3814e-01,  4.0994e-01,  7.0178e-01,  9.5528e-01,\n",
       "          5.5358e-01,  4.0091e-01,  2.3186e-01, -9.7927e-01, -9.8584e-01,\n",
       "         -2.8673e-01, -2.4337e-01,  7.2493e-01,  5.7315e-01,  8.8239e-01,\n",
       "          4.4722e-01, -4.8819e-01,  1.0828e-01, -1.5406e-02, -1.9398e-01,\n",
       "         -9.9173e-01,  4.4130e-01, -3.4468e-01, -9.7585e-01,  9.5458e-01,\n",
       "         -3.4396e-01, -7.9580e-02,  2.5765e-01, -4.4630e-01,  9.6260e-01,\n",
       "          7.0019e-01,  4.7741e-01,  4.4010e-02,  4.4377e-01,  8.6974e-01,\n",
       "          9.5424e-01,  9.8352e-01, -4.9369e-01,  7.9218e-01, -3.5132e-01,\n",
       "          3.8625e-01,  3.8565e-01, -9.2863e-01,  7.0785e-02,  1.6214e-01,\n",
       "         -3.1588e-01,  2.0408e-01, -1.4592e-01, -9.8176e-01,  4.5655e-01,\n",
       "         -2.5013e-01,  5.3519e-01, -3.0179e-01,  1.2085e-01, -3.8113e-01,\n",
       "         -1.1308e-01, -6.9466e-01, -7.3338e-01,  5.4423e-01,  4.4366e-01,\n",
       "          8.8669e-01,  6.6931e-01, -5.6722e-02, -7.1292e-01, -2.4256e-01,\n",
       "         -4.3072e-01, -9.1745e-01,  9.4306e-01, -9.8464e-03, -3.4556e-02,\n",
       "          2.2738e-01, -1.4741e-01,  5.2788e-01, -1.6728e-01, -3.8034e-01,\n",
       "         -3.0925e-01, -6.6327e-01,  8.3470e-01, -4.2990e-02, -4.6003e-01,\n",
       "         -5.9524e-01,  6.2858e-01,  2.4777e-01,  9.9939e-01, -4.3522e-01,\n",
       "         -7.0336e-01, -1.4562e-01, -3.0957e-01,  2.7874e-01, -4.0991e-01,\n",
       "         -1.0000e+00,  3.9205e-01, -1.8185e-01,  4.2810e-01, -5.4864e-01,\n",
       "          3.9862e-01, -5.0947e-01, -9.8203e-01, -1.8075e-01,  7.7716e-02,\n",
       "          4.6108e-01, -4.8077e-01, -6.7799e-01,  4.4749e-01, -4.3147e-02,\n",
       "          8.9809e-01,  8.6203e-01, -1.4951e-01,  4.1630e-01,  5.6037e-01,\n",
       "         -3.0576e-01, -6.4009e-01,  9.1662e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model(**sentence_tokens)\n",
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wzp_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
