{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yxlu/zpwang/InsultingLanguageDetection\n"
     ]
    }
   ],
   "source": [
    "# 选择根目录\n",
    "%cd /home/yxlu/zpwang/InsultingLanguageDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxlu/anaconda3/envs/Learning/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoModel\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    自定义用于分类的模块，输入特征、输出分类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, classifier_dropout=0.1, num_labels=3):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)  # 全连接层，维度不变\n",
    "        self.dropout = nn.Dropout(classifier_dropout)  # Dropout，随机失活，防止过拟合的常用模块\n",
    "        self.out_proj = nn.Linear(hidden_size, num_labels)  # 全连接层，第二维 hidden_size -> num_labels\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        输入特征，自定义前馈方法并返回分类\n",
    "        运算时注意维度变化 (batch_size, sequence_length, hidden_size) -> (batch_size, num_labels)\n",
    "        \"\"\"\n",
    "        features  # batch_size, sequence_length, hidden_size\n",
    "        x = features[:, 0, :]  # batch_size, hidden_size  # 取第一个 token <s> 作为句子的编码\n",
    "        x = self.dropout(x)  # batch_size, hidden_size\n",
    "        x = self.dense(x)  # batch_size, hidden_size\n",
    "        x = torch.tanh(x)  # batch_size, hidden_size\n",
    "        x = self.dropout(x)  # batch_size, hidden_size\n",
    "        x = self.out_proj(x)  # batch_size, num_labels\n",
    "        return x  # batch_size, num_labels\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    \"\"\"\n",
    "    自定义模型结构，处理模型输入得到所需输出\n",
    "    此处以基本的 Encoder-Decoder 架构为例\n",
    "        句子 -> tokenizer -> 预训练语言模型 -> 自定义分类头 -> 分类结果\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 encoder_name='bert-base-uncased',\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.encoder_name = encoder_name\n",
    "        \n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        self.decoder = ClassificationHead(hidden_size=768)  # hidden_size 为对应预训练语言模型的输出维度，bert base为768\n",
    "        # self.model = AutoModelForSequenceClassification.from_pretrained(config.encoder_name, num_labels=2)\n",
    "    \n",
    "    def forward(self, batch_x):\n",
    "        \"\"\"\n",
    "        自定义前馈过程，输入tokenizer分词后的语句，输出各个分类的概率\n",
    "        \"\"\"\n",
    "        feature = self.encoder(**batch_x)\n",
    "        feature = feature['last_hidden_state']\n",
    "        feature  # batch_size, sequence_length, hidden_size\n",
    "        \n",
    "        logits = self.decoder(feature)  # batch_size, num_labels\n",
    "        probs = F.softmax(logits, dim=-1)  # batch_size, num_labels\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, batch_x):\n",
    "        \"\"\"\n",
    "        自定义预测过程，输入tokenizer分词后的语句，输出预测结果（属于哪个分类）\n",
    "        \"\"\"\n",
    "        output = self(batch_x)  \n",
    "        preds = torch.argmax(output, dim=-1)  # 在 probs 的基础上取概率最大的分类\n",
    "        return preds\n",
    "\n",
    "\n",
    "encoder_name = 'bert-base-uncased'\n",
    "model = CustomModel(encoder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.02MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词后的句子 tensor([[ 101, 7592, 2088,  102,    0],\n",
      "        [ 101, 1045, 2066, 2009,  102],\n",
      "        [ 101, 1045, 5223, 2009,  102]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "sentences = [\n",
    "    'Hello world',\n",
    "    'I like it',\n",
    "    'I hate it',\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=encoder_name)\n",
    "sentences_tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "print('分词后的句子', sentences_tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测概率\n",
      "tensor([[0.2819, 0.4500, 0.2681],\n",
      "        [0.3406, 0.4056, 0.2537],\n",
      "        [0.3876, 0.3266, 0.2857]], grad_fn=<SoftmaxBackward0>)\n",
      "预测分类\n",
      "tensor([1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "x = sentences_tokens\n",
    "probs = model(x)\n",
    "preds = model.predict(x)\n",
    "print(f'预测概率\\n{probs}\\n预测分类\\n{preds}')\n",
    "\n",
    "'''\n",
    "由于有dropout，输出结果存在随机性，概率与分类可能对不上\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写于 2023.8.11"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
